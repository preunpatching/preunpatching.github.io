{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/preunpatching/ollama-runtime-for-kaggle?scriptVersionId=226511346\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Ollama Runtime for Kaggle\nThis notebook allows you to quickly run Ollama on Kaggle's servers, which can be useful for most people who do not have a powerful enough PC to run certain models.\n## How to run\n### Start notebook\nIt's recommended that the runtime is configured to use 2x T4 GPUs. To do that, go to Settings > Accelerator and set it to \"GPU T4 x2\". The runtime may run with just a CPU, however this will cause serious performance degredations compared to with a GPU.\nOnce ready, press Ctrl+Shift+Alt+Enter, or go to Runtime > Run all.\n### Start model\nWhen the notebook asks you to select a compatible Ollama model to use, you may go to https://ollama.com/models to see the library of available models. **Be aware that not all models will run on the best free version of the runtime (29 GB RAM, 2x T4 GPU with 32 GB VRAM), so pick the correct model to use!**\n## Bugs\n*   None\n\nOnce done, you're ready to chat with your selected model!\nHave fun!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Check if we're using an Nvidia GPU.\nimport os\nif os.path.isfile(\"/opt/bin/nvidia-smi\"):\n  print(\"Nvidia GPU detected.\")\n  # Install pciutils so that Ollama can auto-detect the Nvidia GPU.\n  !sudo apt install pciutils\nelse:\n  print(\"Couldn't detect Nvidia GPU. This will cause serious performance degredations when using a CPU only. It is recommended that you switch to a Nvidia GPU to prevent performance degredations.\")\n  input(\"Press ENTER to continue . . .\")\n# Install Ollama, as well as its API and BS4 for model search function.\n!pip install ollama bs4\n!curl -fsSL https://ollama.com/install.sh | sh\nprint(\"\\033[0m\", end='')\n# Start Ollama server in a seperate process.\nimport subprocess\nimport signal\nprocess = subprocess.Popen(\"ollama serve\", shell=True)\n# Wait for Ollama server to initialize.\nimport time\ntime.sleep(0.5)\n# Prompt user for which Ollama model to use.\nmodel = \"\"\nwhile not model:\n  model = input(\"Select compatible Ollama model to use: \")\n# Pull selected model.\nfrom tqdm import tqdm\nfrom ollama import pull\ncurrent_digest, bars = '', {}\nfor progress in pull(model, stream=True):\n  digest = progress.get('digest', '')\n  if digest != current_digest and current_digest in bars:\n    bars[current_digest].close()\n\n  if not digest:\n    print(progress.get('status'))\n    continue\n\n  if digest not in bars and (total := progress.get('total')):\n    bars[digest] = tqdm(total=total, desc=f'pulling {digest[7:19]}', unit='B', unit_scale=True)\n\n  if completed := progress.get('completed'):\n    bars[digest].update(completed - bars[digest].n)\n\n  current_digest = digest\n# Start model.\nfrom ollama import chat\nmessages = []\nwhile True:\n  try:\n    user_input = input('>>> ')\n    if user_input.lower() in [\"/bye\"]:\n      print(\"Terminating Ollama server...\")\n      process.send_signal(signal.SIGTERM)\n      try:\n          process.wait(timeout=5)  # Wait up to 5 seconds\n          print(\"Ollama server terminated gracefully.\")\n      except subprocess.TimeoutExpired:\n          print(\"Terminating Ollama server forcefully...\")\n          process.send_signal(signal.SIGKILL) # Force termination\n          process.wait()\n          print(\"Ollama server terminated forcefully.\")\n      break\n    elif user_input.lower() in [\"/clear\"]:\n      messages = []\n      print(\"Cleared session context\")\n    else:\n      response = ''\n      for part in chat(\n        model,\n        messages=messages\n        + [\n          {'role': 'user', 'content': user_input},\n        ],\n        stream=True,\n      ):\n        response = response + part['message']['content']\n        print(part['message']['content'], end='', flush=True)\n\n      # Add the response to the messages to maintain the history.\n      messages += [\n        {'role': 'user', 'content': user_input},\n        {'role': 'assistant', 'content': response},\n      ]\n  except KeyboardInterrupt:\n    print(\"Terminating Ollama server...\")\n    process.send_signal(signal.SIGTERM)\n    try:\n        process.wait(timeout=5)  # Wait up to 5 seconds\n        print(\"Ollama server terminated gracefully.\")\n    except subprocess.TimeoutExpired:\n        print(\"Terminating Ollama server forcefully...\")\n        process.send_signal(signal.SIGKILL) # Force termination\n        process.wait()\n        print(\"Ollama server terminated forcefully.\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T17:58:35.532244Z","iopub.execute_input":"2025-03-08T17:58:35.532777Z","iopub.status.idle":"2025-03-08T18:01:13.004345Z","shell.execute_reply.started":"2025-03-08T17:58:35.532742Z","shell.execute_reply":"2025-03-08T18:01:13.003075Z"}},"outputs":[{"name":"stdout","text":"Couldn't detect Nvidia GPU. This will cause serious performance degredations when using a CPU only. It is recommended that you switch to a Nvidia GPU to prevent performance degredations.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Press ENTER to continue . . . \n"},{"name":"stdout","text":"Requirement already satisfied: ollama in /usr/local/lib/python3.10/dist-packages (0.4.7)\nRequirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\nRequirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.10/dist-packages (from ollama) (0.28.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from ollama) (2.11.0a2)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.29,>=0.27->ollama) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.2)\n>>> Cleaning up old version at /usr/local/lib/ollama\n>>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n############################################################################################# 100.0%                                                       16.3%#######################                                                                     28.4%###############################                                                     45.7%#######################################################                                   65.5%\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n\u001b[0m","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Select compatible Ollama model to use:  \nSelect compatible Ollama model to use:  smollm2\n"},{"name":"stdout","text":"pulling manifest\n","output_type":"stream"},{"name":"stderr","text":"pulling 4d2396b16114: 100%|██████████| 1.82G/1.82G [00:05<00:00, 337MB/s] \npulling fbacade46b4d: 100%|██████████| 68.0/68.0 [00:01<00:00, 57.1B/s]\npulling dfebd0343bdd: 100%|██████████| 1.83k/1.83k [00:01<00:00, 1.56kB/s]\npulling 58d1e17ffe51: 100%|██████████| 11.4k/11.4k [00:01<00:00, 9.74kB/s]\npulling f02dd72bb242: 100%|██████████| 59.0/59.0 [00:01<00:00, 49.7B/s]\npulling 6c6b9193c417: 100%|██████████| 559/559 [00:00<00:00, 564B/s]  \n","output_type":"stream"},{"name":"stdout","text":"verifying sha256 digest\nwriting manifest\nsuccess\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">>>  What is pi?\n"},{"name":"stdout","text":"Pi (π) is a mathematical constant that represents the ratio of a circle's circumference to its diameter. It is approximately equal to 3.14159 but is an irrational number, meaning it cannot be expressed as a simple fraction and its decimal representation goes on indefinitely without repeating. Pi is used in various mathematical formulas, particularly those dealing with circles or spheres, such as calculating the area of a circle (A = πr^2) or the volume of a sphere (V = 4/3πr^3).","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">>>  /bye\n"},{"name":"stdout","text":"Terminating Ollama server...\nOllama server terminated gracefully.\n","output_type":"stream"}],"execution_count":3}]}